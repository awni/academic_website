<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="maintained-by" content="Awni Hannun">
    <meta name="author" content="Awni Hannun">
    <meta name="keywords" content="Awni Hannun">

    <!-- Favicon -->
    <link rel="apple-touch-icon" sizes="180x180" href="../assets/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../assets/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../assets/favicon-16x16.png">
    <link rel="manifest" href="../assets/site.webmanifest">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link href="../assets/styles.css" rel="stylesheet">

    <title>Awni Hannun</title>
  </head>

  <body>
    <div class="container mt-5 mb-5">
      <div class="row">
        <div class="col">
          <h2>Awni Hannun</a></h2>
          <hr>
        </div>
      </div>
      <div class="row pt-md-5">
        <div class="col-md-2 mb-2">
          <nav class="nav flex-column nav-pills">
            <a class="nav-item nav-link" href="../index.html">Home</a>
            <a class="nav-item nav-link" href="../research.html">Research</a>
            <a class="nav-item nav-link" href="../publications.html">Publications</a>
            <a class="nav-item nav-link" href="../writing.html">Writing</a>
            <a class="nav-item nav-link" href="../software.html">Software</a>
            <a class="nav-item nav-link" href="../misc.html">Misc</a>
          </nav>
        </div>
        <div class="col-md-8">
          <main class="shadow p-3 mb-5 bg-white rounded" style="padding:10px">

<h5>Speech Recognition</h5>
<p>This research focuses on more accurate speech recognition with end-to-end
models and scale. In the past I also worked on hybrid HMM-based speech
recognition.</p>

<div class="papers">
<h5>Selected Publications</h5>
<ul>

  <li><strong>Massively Multilingual ASR: 50 Languages, 1 Model, 1 Billion Parameters</strong>
    Vineel Pratap, Anuroop Sriram, Paden Tomasello, Awni Hannun, Vitaliy Liptchinsky, Gabriel Synnaeve, Ronan Collobert. Interspeech 2020.
    (<a href="https://arxiv.org/abs/2007.03001">paper</a>)
  </li>

  <li><strong>Scaling up online speech recognition using convnets,</strong>
    Vineel Pratap, Qiantong Xu, Jacob Kahn, Gilad Avidov, Tatiana Likhomanenko, Awni Hannun, Vitaliy Liptchinsky, Gabriel Synnaeve, Ronan Collobert.
    Interspeech 2020.
    (<a href="https://arxiv.org/abs/2001.09727">paper</a>)
  </li>

  <li><strong>Word-level Speech Recognition with a Letter to Word Encoder</strong>
    Ronan Collobert, Awni Hannun, Gabriel Synnaeve. ICML 2020. 
    (<a href="https://arxiv.org/abs/1906.04323">paper</a>)
  </li>

  <li><strong>Lead2Gold: Towards exploiting the full potential of noisy transcriptions for speech recognition</strong>
    Adrien Dufraux, Emmanuel Vincent, Awni Hannun, Armelle Brun, Matthijs Douze. ASRU 2019. 
    (<a href="https://arxiv.org/abs/1910.07323">paper</a>)
  </li>

  <li><strong>Sequence-to-Sequence Speech Recognition with Time-Depth Separable
      Convolutions,</strong> Awni Hannun, Ann Lee, Qiantong Xu, Ronan
    Collobert. Interspeech 2019.
    (<a href="https://arxiv.org/abs/1904.02619">paper</a>,
    <a href="https://github.com/facebookresearch/wav2letter/tree/master/recipes/models/seq2seq_tds">code</a>)
  </li>

  <li><strong>Wav2Letter++: A Fast Open-source Speech Recognition System,</strong>Vineel Pratap,
    Awni Hannun, Qiantong Xu, Jeff Cai, Jacob Kahn, Gabriel Synnaeve, Vitaliy Liptchinsky,
    Ronan Collobert. ICASSP 2019. (<a href="https://arxiv.org/abs/1812.07625">paper</a>,
    <a href="https://github.com/facebookresearch/wav2letter">code</a>,
    <a href="https://engineering.fb.com/ai-research/wav2letter/">blog</a>)
  </li>

  <li><strong>Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,</strong>
  <a href="http://research.baidu.com/silicon-valley-ai-lab/" target="_blank">SVAIL</a>. ICML 2016. 
  (<a href="http://jmlr.org/proceedings/papers/v48/amodei16.pdf">pdf</a>, <a href="http://arxiv.org/abs/1512.02595">long</a>)
  <br />
  Mentions: <a href="https://www.technologyreview.com/s/600766/conversational-interfaces/" target="_blank">MIT Tech Review</a>,
  <a href="https://www.technologyreview.com/s/544651/baidus-deep-learning-system-rivals-people-at-speech-recognition/" target="_blank">MIT Tech Review</a>
  </li>

  <li><strong>Deep Speech: Scaling up end-to-end speech recognition,</strong>
  Awni Y. Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich
  Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, Andrew
  Y. Ng. arXiv:1412.5567, 2014. 
  (<a href="http://arxiv.org/pdf/1412.5567.pdf">pdf</a>)
  <br />
  Mentions: <a href="http://www.forbes.com/sites/roberthof/2014/12/18/baidu-announces-breakthrough-in-speech-recognition-claiming-to-top-google-and-apple/#6a9652845a11" target="_blank">Forbes</a>
  </li>

	<li><strong>Rectifier Nonlinearities Improve Neural Network
	Acoustic Models,</strong> Andrew L. Maas, Awni Y. Hannun, and Andrew
	Y. Ng. ICML Workshop on Deep Learning for Audio, Speech,
	and Language Processing (WDLASL 2013). (<a href="papers/relu_hybrid_icml2013_final.pdf">pdf</a>)
  </li>

</ul>
</div>


          </main>
        </div>
      </div>
    </div>
  </body>
</html>
